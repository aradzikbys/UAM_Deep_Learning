---
title: 'Deep neural networks -- convolutional neural network'
author: 'Tomasz Górecki'
date: 'Last update: `r format(Sys.Date(), "%d.%m.%Y")`'
output: 
  pdf_document: 
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---

## Libraries
```{r load_keras, message = FALSE, warning = FALSE}
library(keras)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
```

## MNIST data set example

### Description
The MNIST dataset is included with Keras and can be accessed using the 
`r kableExtra::text_spec("dataset_mnist()", color = "blue")` function. The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.17%.

|                  Type                  |                         Classifier                        | Error rate (%) |
|:--------------------------------------:|:---------------------------------------------------------:|:--------------:|
|            Linear classifier           |                 Pairwise linear classifier                |      7.60      |
|          Non-linear classifier         |               40 PCA + quadratic classifier               |      3.30      |
|           Random Forest (RF)           |            Fast Unified Random Forests (RF-SRC)           |      2.80      |
|        Deep neural network (DNN)       |                     2-layer 784-800-10                    |      1.60      |
|             Boosted Stumps             |             Product of stumps on Haar features            |      0.87      |
|        Deep neural network (DNN)       |                     2-layer 784-800-10                    |      0.70      |
|      Support-vector machine (SVM)      |                  Virtual SVM, deg-9 poly                  |      0.56      |
|        K-Nearest Neighbors (KNN)       |              K-NN with non-linear deformation             |      0.52      |
|        Deep neural network (DNN)       |           6-layer 784-2500-2000-1500-1000-500-10          |      0.35      |
|   Convolutional neural network (CNN)   |             6-layer 784-40-80-500-1000-2000-10            |      0.31      |
|   Convolutional neural network (CNN)   |             6-layer 784-50-100-500-1000-10-10             |      0.27      |
|   Convolutional neural network (CNN)   |          Committee of 35 CNNs, 1-20-P-40-P-150-10         |      0.23      |
|   Convolutional neural network (CNN)   |   Committee of 5 CNNs, 6-layer 784-50-100-500-1000-10-10  |      0.21      |
| Random Multimodel Deep Learning (RMDL) |                   10 NN-10 RNN - 10 CNN                   |      0.18      |
|   Convolutional neural network (CNN)   | Committee of 20 CNNS with Squeeze-and-Excitation Networks |      0.17      |

### Loading data set
```{r load_data}
mnist <- dataset_mnist()
c(x_train, y_train) %<-% mnist$train # Train set features, train set labels
c(x_test, y_test) %<-% mnist$test # Test set features, test set labels

dim(x_train) # The dimensions of the training feature set (the images)
dim(x_test)
```

The x data is a 3-d array (images, width, height) of grayscale values. The y data is an integer vector with values ranging from 0 to 9. 

### Data prepare
These images are not in the the correct shape as tensors, as the number of channels is missing.

```{r x_data_prepare}
# Reshape
img_rows <- 28
img_cols <- 28
x_train <- array_reshape(x_train,
                         c(nrow(x_train),
                           img_rows,
            # 1 channel >> since pictures are black and white
            # For color pictures >> 3 canals
                           img_cols, 1))
x_test <- array_reshape(x_test,
                        c(nrow(x_test),
                          img_rows,
                          img_cols, 1))
# Input: 28x28x1 (for colored 28x28x3) instead of 28x28
input_shape <- c(img_rows,
                 img_cols, 1)
```

The data must be normalized. Since the pixel values represent brigness on a scale from 0 (black) to 255 (white), they can all be rescaled by dividing each by the maximum value of 255.
```{r rescale}
# Rescale
x_train <- x_train / 255
x_test <- x_test / 255
```

To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras `r kableExtra::text_spec("to_categorical()", color = "blue")` function. One hot encoding is a vector representation where all elements of the vector are 0 except one, which has 1 as its value (assigning 1 to working feature and 0’s to other idle features).

```{r y_data_prepare}
num_classes <- 10
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
```

9 first encoded digits from trainig set are below (the first column corresponds to zero, second to one, etc.):
```{r}
head(y_train, 9)
```

### Defining the model
```{r model}
model <- keras_model_sequential() %>%
  # convolution 2d - flat images
                # filters - how many kernels?
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3),
                activation = 'relu',
                # 28 x 28 x 1
                input_shape = input_shape) %>% # How many parameters? (3 * 3 * 1 + 1) * 32 = 320
  # (filter_rows * filter_cols * filters_previous_layer + 1) * filters
  # 2 * 32 + 2 * 32 = 128 (2 learnable & 2 non-learnable)
  layer_batch_normalization() %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, 
                kernel_size = c(3, 3), 
                activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% # (3 * 3 * 32 + 1) * 64 = 18 496
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% # 5 * 5 * 64 neurons
  layer_dense(units = 64,
              activation = 'relu') %>% # 1600 * 64 + 64 = 102 464
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = num_classes,
              activation = 'softmax') # 64 * 10 + 10 = 650
```

### Summary the model
A summary of the model shows $122 058$ learnable parameters.
```{r model_summary}
summary(model)
```

### Plot the model
```{r}
devtools::install_github('andrie/deepviz')
deepviz::plot_model(model)
```

### Compile the model
* Loss function -- This measures how accurate the model is during training. We want to minimize this function to 'steer' the model in the right direction.
* Optimizer -- This is how the model is updated based on the data it sees and its loss function.
* Metrics -- Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the digits that are correctly classified.

```{r model_compile}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = 'accuracy')
```

### Training the model
A mini-batch size of 128 will allow the tensors to fit into the memory most of NVidia graphics processing unit. The model will run over 15 epochs, with a validation split set at 0.2.

```{r}
# tensorboard('logs/run_a')
model %>% fit(x_train, 
              y_train, 
              epochs = 15, # Number of epochs
              batch_size = 128, # Size of batch in single step
              validation_split = 0.2 # Percent of data in validation sets
              # callbacks = callback_tensorboard('logs/run_a')
) -> model_cnn 
# 375 = 0.8 * 60000 / 128
plot(model_cnn)
```

### Evaluate the model
```{r}
model %>% evaluate(x_train, y_train, verbose = 0) # Evaluate the model’s performance on the train data
model %>% evaluate(x_test, y_test, verbose = 0) # Evaluate the model’s performance on the test data
```

### Predictions
```{r}
model %>% predict(x_test) -> predictions # Predicted probabilities on test data
model %>% 
  predict(x_test) %>% 
  k_argmax() %>% 
  as.numeric() -> predicted_digits # Predicted digits on test data
```

A prediction is an array of 10 numbers. These describe the 'confidence' of the model that the image corresponds to each of the 10 different digits. Let's plot several images with their predictions. Correct prediction labels are green and incorrect prediction labels are red.

```{r}
par(mfcol = c(5, 5))
par(mar = c(0, 0, 1.5, 0), xaxs = 'i', yaxs = 'i')
for (i in 1:25) { 
  img <- mnist$test$x[i, , ]
  img <- t(apply(img, 2, rev))
  if (predicted_digits[i] == mnist$test$y[i]) {
    color <- '#008800' 
  } else {
    color <- '#bb0000'
  }
  image(1:28, 1:28, img, col = gray((255:0) / 255), xaxt = 'n', yaxt = 'n',
        main = paste0(predicted_digits[i], ' (',
                      mnist$test$y[i], ')'),
        col.main = color)
}
```

### Confusion matrix
```{r}
data.frame(table(predicted_digits, mnist$test$y)) %>% 
  setNames(c('Prediction', 'Reference', 'Freq')) %>% 
  mutate(GoodBad = ifelse(Prediction == Reference, 'Correct', 'Incorrect')) -> conf_table

conf_table %>% 
  ggplot(aes(y = Reference, x = Prediction, fill = GoodBad, alpha = Freq)) + 
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 0.5, fontface  = 'bold', alpha = 1) + 
  scale_fill_manual(values = c(Correct = 'green', Incorrect = 'red')) +
  guides(alpha = 'none') + 
  theme_bw() + 
  ylim(rev(levels(conf_table$Reference)))
```