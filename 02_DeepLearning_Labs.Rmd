---
title: 'Deep neural networks -- laboratory'
author: 'Tomasz Górecki'
date: 'Last update: `r format(Sys.Date(), "%d.%m.%Y")`'
output: pdf_document
editor_options: 
  chunk_output_type: console
---

## Installation
1. Library installation
```{r install, eval=FALSE, include=TRUE}
install.packages('keras')
```

2. The Keras R interface uses the [\underline{TensorFlow}](https://www.tensorflow.org) backend engine by default. To install both the core Keras library as well as the TensorFlow backend:

```{r load_keras, message=FALSE, warning=FALSE}
library(keras)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
```

```{r install_tf, eval = FALSE, include = TRUE}
install_keras()
```

## MNIST data set (Example -- classification)

### Description
The MNIST dataset is included with Keras and can be accessed using the 
`r kableExtra::text_spec("dataset_mnist()", color = "blue")` function. The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.17%.

|                  Type                  |                         Classifier                        | Error rate (%) |
|:--------------------------------------:|:---------------------------------------------------------:|:--------------:|
|            Linear classifier           |                 Pairwise linear classifier                |      7.60      |
|          Non-linear classifier         |               40 PCA + quadratic classifier               |      3.30      |
|           Random Forest (RF)           |            Fast Unified Random Forests (RF-SRC)           |      2.80      |
|        Deep neural network (DNN)       |                     2-layer 784-800-10                    |      1.60      |
|             Boosted Stumps             |             Product of stumps on Haar features            |      0.87      |
|        Deep neural network (DNN)       |                     2-layer 784-800-10                    |      0.70      |
|      Support-vector machine (SVM)      |                  Virtual SVM, deg-9 poly                  |      0.56      |
|        K-Nearest Neighbors (KNN)       |              K-NN with non-linear deformation             |      0.52      |
|        Deep neural network (DNN)       |           6-layer 784-2500-2000-1500-1000-500-10          |      0.35      |
|   Convolutional neural network (CNN)   |             6-layer 784-40-80-500-1000-2000-10            |      0.31      |
|   Convolutional neural network (CNN)   |             6-layer 784-50-100-500-1000-10-10             |      0.27      |
|   Convolutional neural network (CNN)   |          Committee of 35 CNNs, 1-20-P-40-P-150-10         |      0.23      |
|   Convolutional neural network (CNN)   |   Committee of 5 CNNs, 6-layer 784-50-100-500-1000-10-10  |      0.21      |
| Random Multimodel Deep Learning (RMDL) |                   10 NN-10 RNN - 10 CNN                   |      0.18      |
|   Convolutional neural network (CNN)   | Committee of 20 CNNS with Squeeze-and-Excitation Networks |      0.17      |

### Loading data set
```{r load_data}
mnist <- dataset_mnist()
# %<-% operator will split mnist$train into  2 elements: x_train, y_train
c(x_train, y_train) %<-% mnist$train # Train set features, train set labels
# same for test (so we will not have lists, but matrix)
c(x_test, y_test) %<-% mnist$test # Test set features, test set labels
```

The x data is a 3-d array (images, width, height) of grayscale values. The y data is an integer vector with values ranging from 0 to 9. 

### Visualize the 9 first digits from training set

```{r, visualize_digits}
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) { 
  # x train has 3 dimensions, i-image and all columns and rows
  im <- x_train[i,,]
  # reverse colors
  im <- t(apply(im, 2, rev)) 
  #image in greyscale
  image(1:28, 1:28, im, col = gray((255:0) / 255), 
        xaxt = 'n', main = paste(y_train[i]))
}
```

### Data prepare
To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1

```{r x_data_prepare}
# Reshape to vector (vs 28x28 matrix)
x_train <- array_reshape(x_train, c(nrow(x_train), 28 * 28))
x_test <- array_reshape(x_test, c(nrow(x_test), 28 * 28))
# Rescale from 0-255 to 0-1
x_train <- x_train / 255
x_test <- x_test / 255
```

To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras `r kableExtra::text_spec("to_categorical()", color = "blue")` function. One hot encoding is a vector representation where all elements of the vector are 0 except one, which has 1 as its value (assigning 1 to working feature and 0’s to other idle features).

```{r y_data_prepare}
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

9 first encoded digits from trainig set are below (the first column corresponds to zero, second to one, etc.):
```{r}
head(y_train, 9)
```

### Defining the model
The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers.

```{r model}
# start with sequentional model >> we add layers. It has 28x28 options
model <- keras_model_sequential()
model %>% 
  # 1st layer >> DENSE.
  # Units >> number of neurons, good practice to keep 2^n format
  # activation = 'relu' >> (change negatives to 0, keep positive numbers as they are)
  # input shape = how many pixels? given in 1st layer only
  layer_dense(units = 256, activation = 'relu', input_shape = 28 * 28) %>% 
  # we exclude 40% of neurons before we will add new layer (not to overlearn), random dropout
  layer_dropout(rate = 0.4) %>% 
  # next dense layer, input not needed
  layer_dense(units = 128, activation = 'relu') %>%
  # deactivate 30%
  layer_dropout(rate = 0.3) %>%
  # last layer - units = 10, because we have 10 classes to find (0-9)
  # softmax is recommended for last layer >> we will get probability vector
  layer_dense(units = 10, activation = 'softmax')
```
The input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function. Softmax activation function takes as input a vector of $K$ real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers $$f_i(\pmb z) = \frac{e^{z_{i}}}{\sum_{j=1}^Ke^{z_{j}}}.$$

### Summary the model
```{r model_summary}
summary(model)
```

### Compile the model
* Loss function -- This measures how accurate the model is during training. We want to minimize this function to 'steer' the model in the right direction.
* Optimizer -- This is how the model is updated based on the data it sees and its loss function.
* Metrics -- Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the digits that are correctly classified.

```{r model_compile}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = 'accuracy')
```

### Training the model
```{r}
model %>% fit(x_train, 
              y_train, 
              # Number of epochs
              epochs = 50, 
              # Size of batch in single step
              # you can learn network showing data in different granularity
              # you can run 1 observation and then adjust parameters/weight (too slow),
              # you can run all observations >> then it won't be accurate
              # batch: you specify number of observation run through network
              batch_size = 128, 
              # take 20% from test dataset to validate data
              validation_split = 0.2 # Percent of data in validation sets
) -> model_dnn 
# when validation/train cross >> we get number of epochs
plot(model_dnn)
```

### Evaluate the model
```{r}
model %>% evaluate(x_train, y_train) # Evaluate the model’s performance on the train data
model %>% evaluate(x_test, y_test) # Evaluate the model’s performance on the test data
```

It turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data

### Predictions
```{r}
model %>% predict(x_test) -> predictions # Predicted probabilities on test data
model %>% predict(x_test) %>% k_argmax() %>% as.numeric() -> predicted_digits # Predicted digits on test data
```

A prediction is an array of 10 numbers. These describe the 'confidence' of the model that the image corresponds to each of the 10 different digits. Let's plot several images with their predictions. Correct prediction labels are green and incorrect prediction labels are red.

```{r}
par(mfcol = c(5, 5))
par(mar = c(0, 0, 1.5, 0), xaxs = 'i', yaxs = 'i')
for (i in 1:25) { 
  img <- mnist$test$x[i, , ]
  img <- t(apply(img, 2, rev))
  if (predicted_digits[i] == mnist$test$y[i]) {
    color <- '#008800' 
  } else {
    color <- '#bb0000'
  }
  image(1:28, 1:28, img, col = gray((255:0) / 255), xaxt = 'n', yaxt = 'n',
                      # prediction 
        main = paste0(predicted_digits[i], ' (',
                      # (real)
                      mnist$test$y[i], ')'),
        col.main = color)
}
```

### Confusion matrix
```{r}
data.frame(table(predicted_digits, mnist$test$y)) %>% 
  setNames(c('Prediction', 'Reference', 'Freq')) %>% 
  mutate(GoodBad = ifelse(Prediction == Reference, 'Correct', 'Incorrect')) -> conf_table

# columns - predictions, rows - real
conf_table %>% 
  ggplot(aes(y = Reference, x = Prediction, fill = GoodBad, alpha = Freq)) + 
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 0.5, fontface  = 'bold', alpha = 1) + 
  scale_fill_manual(values = c(Correct = 'green', Incorrect = 'red')) +
  guides(alpha = 'none') + 
  theme_bw() + 
  ylim(rev(levels(conf_table$Reference)))
```

## Boston Housing Prices data set (Example -- regression)

### Description
Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. It has 506 total examples that are split between 404 training examples and 102 test examples. The attributes are defined as follows: 

1. CRIM: per capita crime rate by town
2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.
3. INDUS: proportion of non-retail business acres per town
4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
5. NOX: nitric oxides concentration (parts per 10 million)
6. RM: average number of rooms per dwelling
7. AGE: proportion of owner-occupied units built prior to 1940
8. DIS: weighted distances to five Boston employment centers
9. RAD: index of accessibility to radial highways
10. TAX: full-value property-tax rate per $10,000
11. PTRATIO: pupil-teacher ratio by town 12. 
12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town 
13. LSTAT: % lower status of the population
14. MEDV: Median value of owner-occupied homes in $1000s

Each one of these input data features is stored using a different scale. Some features are represented by a proportion between 0 and 1, other features are ranges between 1 and 12, some are ranges between 0 and 100, and so on.

### Loading data
```{r}
boston_housing <- dataset_boston_housing()

c(train_data, train_labels) %<-% boston_housing$train
c(test_data, test_labels) %<-% boston_housing$test
```

```{r}
column_names <- c('CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 
                  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT')
as_tibble(train_data) %>% 
  setNames(column_names) %>% 
  head() %>% 
  knitr::kable('latex', align = rep('c', 10))
train_labels[1:10] # The labels are the house prices in thousands of dollars.
```

### Prepare data
It’s recommended to normalize features that use different scales and ranges. Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model more dependant on the choice of units used in the input.

```{r}
# Normalize training data
train_data <- scale(train_data)

# Test data is not used when calculating the mean and std.
# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center") 
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)
```

### Defining the model

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu",
              input_shape = dim(train_data)[2]) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1)
```

### Summary the model
```{r}
summary(model)
```

### Compile the model
```{r}
model %>% compile(loss = 'mse',
  optimizer = optimizer_adam(),
  metrics = list('mean_absolute_error'))
```
* Mean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems): 
$$MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2,$$ where $y_i$ are true values, a $\hat{y}_i$ are predicted values and $n$ is a sample size.
* Similarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE):
$$MAE = \frac{1}{n}\sum_{i=1}^n|y_i - \hat{y}_i|.$$

### Training the model
```{r}
model %>% fit(
  train_data,
  train_labels,
  epochs = 500,
  validation_split = 0.2) -> model_reg
```

```{r}
plot(model_reg, smooth = FALSE)
```
This graph shows little improvement in the model after about 200 epochs. 

### Evaluate the model
```{r}
model %>% evaluate(train_data, train_labels) # Evaluate the model’s performance on the train data
model %>% evaluate(test_data, test_labels) # Evaluate the model’s performance on the test data
```

### Predictions
```{r}
model %>% predict(test_data) -> predictions # Predicted prices on test data
```
