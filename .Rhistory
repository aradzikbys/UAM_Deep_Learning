# AUC
h2o.auc(prostate.glm)
h2o.performance(prostate.glm)
# Confusion matrix
h2o.confusionMatrix(prostate.glm)
h2o.performance(prostate.glm)
# Pine age based on its height
# Regression
Loblolly.data <- Loblolly[, 1:2]
# convert to h20 format
Loblolly.hex <- as.h2o(Loblolly.data, destination_frame = 'Loblolly.hex')
Loblolly.glm <- h2o.glm(y = 1,
x = 2,
training_frame = Loblolly.hex,
nfolds = 10) # Regression
lm.predict <- h2o.predict(object = Loblolly.glm,
newdata = Loblolly.hex)
Loblolly.glm <- h2o.glm(y = 1,
x = 2,
training_frame = Loblolly.hex,
nfolds = 10) # Regression
lm.predict <- h2o.predict(object = Loblolly.glm,
newdata = Loblolly.hex)
lm.predict
# Basic AutoML
# The leader model
Loblolly.automl <- h2o.automl(y = 1,
x = 2,
training_frame = Loblolly.hex,
max_runtime_secs = 60)
Loblolly.automl@leader
h2o.init(nthreads = -1,
max_mem_size = '4G')
Loblolly.automl@leader
Loblolly.data <- Loblolly[, 1:2]
# Convert to h20 format (visible as Loblolly.hex in browser/h20)
Loblolly.hex <- as.h2o(Loblolly.data, destination_frame = 'Loblolly.hex')
# Regression
Loblolly.glm <- h2o.glm(y = 1,
x = 2,
training_frame = Loblolly.hex,
nfolds = 10)
lm.predict <- h2o.predict(object = Loblolly.glm,
newdata = Loblolly.hex)
# Basic AutoML
# The leader model
Loblolly.automl <- h2o.automl(y = 1,
x = 2,
training_frame = Loblolly.hex,
# we need to specify how long learning will last
max_runtime_secs = 60)
h2o.init(nthreads = -1)
train_file <- 'https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz'
test_file <- 'https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz'
train <- h2o.importFile(train_file) # 13MB
test <- h2o.importFile(test_file) # 2.1MB
# To see a brief summary of the data
summary(train)
summary(test)
# Specify the response and predictor columns
y <- 'C785'
x <- setdiff(names(train), y)
# Encode the response column as categorical for classification
train[, y] <- as.factor(train[, y])
test[, y] <- as.factor(test[, y])
model <- h2o.deeplearning(x = x,
y = y,
training_frame = train,
# test, train, validation sets are needed in deep learning
# in each iteration classification will be better
# validation set >> when quality is dropping
# that means that it's overlearning
validation_frame = test,
distribution = 'multinomial',
activation = 'RectifierWithDropout',
hidden = c(200, 200, 200),
# prevents overlearning
input_dropout_ratio = 0.2,
l1 = 1e-5,
# how many times data will be shown to the network
epochs = 10,
# return metrix about variables imporatnce
variable_importances = TRUE)
model@parameters # View the specified parameters
# Display all performance metrics
model
# Training set metrics
h2o.performance(model, train = TRUE)
# Validation set metrics
h2o.performance(model, valid = TRUE)
# Get MSE only
h2o.mse(model, valid = TRUE)
h2o.varimp(model) # Variable importance
# Variable importance
h2o.varimp(model)
pred <- h2o.predict(model, newdata = test) # Predictions
head(pred)
test[,'C785']
# Grid search
hidden_opt <- list(c(200, 200), c(100, 300, 100), c(500, 500, 500))
l1_opt <- c(1e-5, 1e-7)
hyper_params <- list(hidden = hidden_opt, l1 = l1_opt)
model_grid <- h2o.grid('deeplearning',
hyper_params = hyper_params,
x = x,
y = y,
distribution = 'multinomial',
training_frame = train,
epochs = 5,
validation_frame = test,
search_criteria = list(strategy = 'RandomDiscrete',
max_runtime_secs = 900))
# Print out all prediction errors of the models
summary(model_grid)
# Print out the Test MSE for all of the models
lapply(model_grid@model_ids,
function(model_id)
paste("Test set MSE:",
round(h2o.mse(h2o.getModel(model_id),
valid = TRUE), 6)))
h2o.shutdown(prompt = TRUE) # Shut down the specified instance. All data will be lost.
max_mem_size = '4G')
h2o.init(nthreads = -1,
max_mem_size = '4G')
cwd
mushrooms <- read.csv('mushroom.csv')
mushrooms <- read.csv(file = 'mushroom.csv')
getwd()
getwd()
setwd('C:/Users/User/OneDrive/Edu/Deep Learning')
getwd()
mushrooms <- read.csv(file = 'mushroom.csv')
mushrooms
mushrooms.hex <- as.h2o(mushrooms, destination_frame = 'mushrooms.hex')
mushrooms <- read.csv(file = 'mushroom.csv')
mushrooms.hex <- as.h2o(mushrooms, destination_frame = 'mushrooms.hex')
mushrooms.hex
mushrooms.split <- h2o.splitFrame(data = mushrooms.hex, ratios = 0.75)
mushrooms_test <- mushrooms.split[[1]]
mushrooms_train <- mushrooms.split[[2]]
mushrooms_test
mushrooms_train
# NEW DATA
prostate.hex <- h2o.importFile(path = 'https://raw.github.com/h2oai/h2o/master/smalldata/logreg/prostate.csv',
destination_frame = 'prostate.hex')
prostate.hex
mushroom_model <- h2o.deeplearning(y = 23,
x = 1:22,
training_frame = mushrooms_train,
distribution = 'multinomial',
activation = 'RectifierWithDropout',
hidden = c(300, 300, 300),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 15,
variable_importances = TRUE)
mushrooms.hex <- as.h2o(mushrooms, destination_frame = 'mushrooms.hex')
mushrooms <- read.csv(file = 'mushroom.csv')
mushrooms <- read.csv(file = 'mushroom.csv')
library(dplyr)
# convert to factors
mushrooms %>%
mutate(across(everything(), as.factor))
mushrooms
skimr::skim(mushrooms)
# convert to factors
mushrooms <- mushrooms %>%
mutate(across(everything(), as.factor))
skimr::skim(mushrooms)
mushrooms.hex <- as.h2o(mushrooms, destination_frame = 'mushrooms.hex')
mushrooms.split <- h2o.splitFrame(data = mushrooms.hex, ratios = 0.75)
mushrooms_test <- mushrooms.split[[1]]
mushrooms_train <- mushrooms.split[[2]]
mushroom_model <- h2o.deeplearning(y = 23,
x = 1:22,
training_frame = mushrooms_train,
distribution = 'multinomial',
activation = 'RectifierWithDropout',
hidden = c(300, 300, 300),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 15,
variable_importances = TRUE)
h2o.shutdown(prompt = TRUE)
h2o.init(nthreads = -1,
max_mem_size = '4G')
mushrooms <- read.csv(file = 'mushroom.csv')
# convert to factors
mushrooms <- mushrooms %>%
mutate(across(everything(), as.factor))
skimr::skim(mushrooms)
mushrooms.hex <- as.h2o(mushrooms, destination_frame = 'mushrooms.hex')
mushrooms.split <- h2o.splitFrame(data = mushrooms.hex, ratios = 0.75)
mushrooms_test <- mushrooms.split[[1]]
mushrooms_train <- mushrooms.split[[2]]
mushrooms <- read.csv(file = 'mushroom.csv')
# convert to factors
mushrooms <- mushrooms %>%
mutate(across(everything(), as.factor))
mushrooms.hex <- as.h2o(mushrooms, destination_frame = 'mushrooms.hex')
mushrooms.split <- h2o.splitFrame(data = mushrooms.hex, ratios = 0.75)
mushrooms_test <- mushrooms.split[[1]]
mushrooms_train <- mushrooms.split[[2]]
mushroom_model <- h2o.deeplearning(y = 23,
x = 1:22,
training_frame = mushrooms_train,
distribution = 'multinomial',
activation = 'RectifierWithDropout',
hidden = c(300, 300, 300),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 15,
variable_importances = TRUE)
mushroom_model <- h2o.deeplearning(
y = 23,
x = 1:22,
training_frame = mushrooms_train,
validation_frame = mushrooms_test,
distribution = 'multinomial',
activation = 'RectifierWithDropout',
hidden = c(300, 300, 300),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 15,
variable_importances = TRUE)
mushroom_model <- h2o.deeplearning(
y = 23,
x = 1:22,
training_frame = mushrooms_train,
validation_frame = mushrooms_test,
distribution = 'multinomial',
activation = 'RectifierWithDropout',
hidden = c(300, 300, 300),
input_dropout_ratio = 0.2,
l1 = 1e-5,
epochs = 60,
variable_importances = TRUE)
h2o.predict(object = mushroom_model,
newdata = mushrooms_test)
mushrooms_test[,1]
mushrooms_test[1,]
h2o.confusionMatrix(mushroom_model)
mushrooms_train
h2o.predict(object = mushroom_model,
newdata = mushrooms_test)
h2o.predict(object = mushroom_model,
newdata = mushrooms_test)
# Mushrooms predictions
mushrom_pred <- h2o.predict(object = mushroom_model,
newdata = mushrooms_test)
mushrom_pred
mushrooms_test[,23]
as.number(mushrom_pred)
# Mushrooms predictions
mushroom_pred <- h2o.predict(object = mushroom_model,
newdata = mushrooms_test)
mushroom_pred
# Table
table(mushroom_pred, mushrooms_test)
# Mushrooms predictions
mushroom_pred <- h2o.predict(object = mushroom_model,
newdata = mushrooms_test)
mushroom_pred
# Table
table(mushroom_pred[,1], mushrooms_test[,23])
mushrooms_test[,23]
mushroom_pred[,1]
mushrooms_test[,23]
mushroom_pred[,1]
as.factor(mushrooms_test[,23])
# Table
table(as.factor(mushrooms_test[,23]),
as.factor(mushroom_pred[,1])
)
# Table
table(as.numeric(mushrooms_test[,23]),
as.numeric(mushroom_pred[,1])
)
# Table
table(as.data.frame(mushrooms_test[,23]),
as.data.frame(mushroom_pred[,1])
)
mushrooms_train[,23]
# Table
h20.table(mushrooms_test[,23],mushroom_pred[,1])
# Table
h20.table(mushrooms_test[,23],mushroom_pred[,1])
# Table
h2o.table(mushrooms_test[,23],mushroom_pred[,1])
h2o.performance(mushroom_model, valid = TRUE)
as.data.frame(mushrooms_test[,23])
# Table
table(as.data.frame(mushrooms_test[,23])[,1],
as.data.frame(mushroom_pred[,1])[,1])
library(caret)
model.ml <- train(Species ~ .,
data = iris,
model = 'rf')
model.ml$finalModel
library(DALEX)
install.packages(DALEX)
install.packages('DALEX')
library(DALEX)
rf.explainer <- explain(model.ml$finalModel,
data = iris,
y = iris$Species)
model_performance(rf.explainer)
# Shape values
rf.sh <- predict_parts(rf.explainer,
iris[1,],
type = 'shap')
plot(rf.sh, show_boxplots = FALSE)
h2o.shutdown(prompt = TRUE)
y
install.packages('keras')
library(keras)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
install_keras()
Y
mnist <- dataset_mnist()
c(x_train, y_train) %<-% mnist$train # Train set features, train set labels
c(x_test, y_test) %<-% mnist$test # Test set features, test set labels
mnist <- dataset_mnist()
c(x_train, y_train) %<-% mnist$train # Train set features, train set labels
c(x_test, y_test) %<-% mnist$test # Test set features, test set labels
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) {
im <- x_train[i,,]
im <- t(apply(im, 2, rev))
image(1:28, 1:28, im, col = gray((255:0) / 255),
xaxt = 'n', main = paste(y_train[i]))
}
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) {
im <- x_train[i,,]
im <- t(apply(im, 2, rev))
image(1:28, 1:28, im, col = gray((255:0) / 255),
xaxt = 'n', main = paste(y_train[i]))
}
# Reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 28 * 28))
x_test <- array_reshape(x_test, c(nrow(x_test), 28 * 28))
# Rescale
x_train <- x_train / 255
x_test <- x_test / 255
mnist$train
x_test
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) {
# x train has 3 dimensions, i-image and all columns and rows
im <- x_train[i,,]
# reverse colors
im <- t(apply(im, 2, rev))
#image in greyscale
image(1:28, 1:28, im, col = gray((255:0) / 255),
xaxt = 'n', main = paste(y_train[i]))
}
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) {
# x train has 3 dimensions, i-image and all columns and rows
im <- x_train[i,,]
# reverse colors
im <- t(apply(im, 2, rev))
#image in greyscale
image(1:28, 1:28, im, col = gray((255:0) / 255),
xaxt = 'n', main = paste(y_train[i]))
}
mnist <- dataset_mnist()
# %<-% operator will split mnist$train into  2 elements: x_train, y_train
c(x_train, y_train) %<-% mnist$train # Train set features, train set labels
# same for test (so we will not have lists, but matrix)
c(x_test, y_test) %<-% mnist$test # Test set features, test set labels
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) {
# x train has 3 dimensions, i-image and all columns and rows
im <- x_train[i,,]
# reverse colors
im <- t(apply(im, 2, rev))
#image in greyscale
image(1:28, 1:28, im, col = gray((255:0) / 255),
xaxt = 'n', main = paste(y_train[i]))
}
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) {
# x train has 3 dimensions, i-image and all columns and rows
im <- x_train[i,,]
# reverse colors
im <- t(apply(im, 2, rev))
#image in greyscale
image(1:28, 1:28, im, col = gray((255:0) / 255),
xaxt = 'n', main = paste(y_train[i]))
}
par(mfcol = c(3, 3)) # Split the screen
par(mar = c(0, 0, 3, 0), xaxs = 'i', yaxs = 'i') # Margins
for (i in 1:9) {
# x train has 3 dimensions, i-image and all columns and rows
im <- x_train[i,,]
# reverse colors
im <- t(apply(im, 2, rev))
#image in greyscale
image(1:28, 1:28, im, col = gray((255:0) / 255),
xaxt = 'n', main = paste(y_train[i]))
}
# Reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 28 * 28))
x_test <- array_reshape(x_test, c(nrow(x_test), 28 * 28))
# Rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
head(y_train, 9)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = 28 * 28) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(),
metrics = 'accuracy')
model %>% fit(x_train,
y_train,
epochs = 50, # Number of epochs
batch_size = 128, # Size of batch in single step
validation_split = 0.2 # Percent of data in validation sets
) -> model_dnn
plot(model_dnn)
model %>% evaluate(x_train, y_train) # Evaluate the model’s performance on the train data
model %>% evaluate(x_test, y_test) # Evaluate the model’s performance on the test data
model %>% predict(x_test) -> predictions # Predicted probabilities on test data
model %>% predict_classes(x_test) -> predicted_digits # Predicted digits on test data
model %>% predict(x_test) %>% k_argmax()-> predictions # Predicted probabilities on test data
model %>% predict_classes(x_test) %>% k_argmax()-> predicted_digits # Predicted digits on test data
model %>% predict(x_test) -> predictions # Predicted probabilities on test data
model %>% predict(x_test) %>% k_argmax() -> predicted_digits # Predicted digits on test data
par(mfcol = c(5, 5))
par(mar = c(0, 0, 1.5, 0), xaxs = 'i', yaxs = 'i')
for (i in 1:25) {
img <- mnist$test$x[i, , ]
img <- t(apply(img, 2, rev))
if (predicted_digits[i] == mnist$test$y[i]) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((255:0) / 255), xaxt = 'n', yaxt = 'n',
main = paste0(predicted_digits[i], ' (',
mnist$test$y[i], ')'),
col.main = color)
}
model %>% predict(x_test) %>% k_argmax() %>% as.numeric() -> predicted_digits # Predicted digits on test data
model %>% predict(x_test) -> predictions # Predicted probabilities on test data
model %>% predict(x_test) %>% k_argmax() %>% as.numeric() -> predicted_digits # Predicted digits on test data
par(mfcol = c(5, 5))
par(mar = c(0, 0, 1.5, 0), xaxs = 'i', yaxs = 'i')
for (i in 1:25) {
img <- mnist$test$x[i, , ]
img <- t(apply(img, 2, rev))
if (predicted_digits[i] == mnist$test$y[i]) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((255:0) / 255), xaxt = 'n', yaxt = 'n',
main = paste0(predicted_digits[i], ' (',
mnist$test$y[i], ')'),
col.main = color)
}
data.frame(table(predicted_digits, mnist$test$y)) %>%
setNames(c('Prediction', 'Reference', 'Freq')) %>%
mutate(GoodBad = ifelse(Prediction == Reference, 'Correct', 'Incorrect')) -> conf_table
conf_table %>%
ggplot(aes(y = Reference, x = Prediction, fill = GoodBad, alpha = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), vjust = 0.5, fontface  = 'bold', alpha = 1) +
scale_fill_manual(values = c(Correct = 'green', Incorrect = 'red')) +
guides(alpha = FALSE) +
theme_bw() +
ylim(rev(levels(conf_table$Reference)))
data.frame(table(predicted_digits, mnist$test$y)) %>%
setNames(c('Prediction', 'Reference', 'Freq')) %>%
mutate(GoodBad = ifelse(Prediction == Reference, 'Correct', 'Incorrect')) -> conf_table
conf_table %>%
ggplot(aes(y = Reference, x = Prediction, fill = GoodBad, alpha = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), vjust = 0.5, fontface  = 'bold', alpha = 1) +
scale_fill_manual(values = c(Correct = 'green', Incorrect = 'red')) +
guides(alpha = none) +
theme_bw() +
ylim(rev(levels(conf_table$Reference)))
data.frame(table(predicted_digits, mnist$test$y)) %>%
setNames(c('Prediction', 'Reference', 'Freq')) %>%
mutate(GoodBad = ifelse(Prediction == Reference, 'Correct', 'Incorrect')) -> conf_table
conf_table %>%
ggplot(aes(y = Reference, x = Prediction, fill = GoodBad, alpha = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), vjust = 0.5, fontface  = 'bold', alpha = 1) +
scale_fill_manual(values = c(Correct = 'green', Incorrect = 'red')) +
guides(alpha = 'none') +
theme_bw() +
ylim(rev(levels(conf_table$Reference)))
